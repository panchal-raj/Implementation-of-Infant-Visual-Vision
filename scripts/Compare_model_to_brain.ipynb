{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NED Compare model to Brain Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = \"jupyter_notebook\" # @param [\"colab\", \"jupyter_notebook\"] {allow-input: true}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/gifale95/NED.git\n",
      "  Cloning https://github.com/gifale95/NED.git to /private/var/folders/sy/39dkvhds36d6q24cvfwkcymc0000gn/T/pip-req-build-jm8jnjv1\n",
      "  Running command git clone --quiet https://github.com/gifale95/NED.git /private/var/folders/sy/39dkvhds36d6q24cvfwkcymc0000gn/T/pip-req-build-jm8jnjv1\n",
      "  Resolved https://github.com/gifale95/NED.git to commit cc238e51fdd117031d1096f104e712d0a4a95f46\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: h5py in ./env/lib/python3.9/site-packages (from NED==0.2.3) (3.12.1)\n",
      "Collecting joblib (from NED==0.2.3)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting nibabel (from NED==0.2.3)\n",
      "  Using cached nibabel-5.3.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.9/site-packages (from NED==0.2.3) (1.26.4)\n",
      "Requirement already satisfied: Pillow in ./env/lib/python3.9/site-packages (from NED==0.2.3) (11.1.0)\n",
      "Requirement already satisfied: scipy in ./env/lib/python3.9/site-packages (from NED==0.2.3) (1.13.1)\n",
      "Collecting scikit-learn (from NED==0.2.3)\n",
      "  Using cached scikit_learn-1.6.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: torch in ./env/lib/python3.9/site-packages (from NED==0.2.3) (2.2.2)\n",
      "Requirement already satisfied: torchvision in ./env/lib/python3.9/site-packages (from NED==0.2.3) (0.17.2)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.9/site-packages (from NED==0.2.3) (4.67.1)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in ./env/lib/python3.9/site-packages (from nibabel->NED==0.2.3) (6.5.2)\n",
      "Requirement already satisfied: packaging>=20 in ./env/lib/python3.9/site-packages (from nibabel->NED==0.2.3) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in ./env/lib/python3.9/site-packages (from nibabel->NED==0.2.3) (4.12.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->NED==0.2.3)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.9/site-packages (from torch->NED==0.2.3) (3.17.0)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.9/site-packages (from torch->NED==0.2.3) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.9/site-packages (from torch->NED==0.2.3) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.9/site-packages (from torch->NED==0.2.3) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.9/site-packages (from torch->NED==0.2.3) (2024.12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./env/lib/python3.9/site-packages (from importlib-resources>=5.12->nibabel->NED==0.2.3) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.9/site-packages (from jinja2->torch->NED==0.2.3) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.9/site-packages (from sympy->torch->NED==0.2.3) (1.3.0)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached nibabel-5.3.2-py3-none-any.whl (3.3 MB)\n",
      "Using cached scikit_learn-1.6.1-cp39-cp39-macosx_10_9_x86_64.whl (12.1 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: NED\n",
      "  Building wheel for NED (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for NED: filename=NED-0.2.3-py3-none-any.whl size=47191 sha256=e7fcf9b09e9415d923a908b70995971efa3acead548a8899fc38f723ac76f966\n",
      "  Stored in directory: /private/var/folders/sy/39dkvhds36d6q24cvfwkcymc0000gn/T/pip-ephem-wheel-cache-bvwox6y4/wheels/9d/fd/4d/b25cf9910b4996a79b4d35d5dc4c5de9fa76deecf79742c13c\n",
      "Successfully built NED\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, nibabel, NED\n",
      "Successfully installed NED-0.2.3 joblib-1.4.2 nibabel-5.3.2 scikit-learn-1.6.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U git+https://github.com/gifale95/NED.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from ned.ned import NED\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms as trn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import spearmanr, pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform == 'jupyter_notebook':\n",
    "    ned_dir = './neural_encoding_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = os.path.join(ned_dir, 'ned_tutorials', 'tutorial_images')\n",
    "model_dir = './cvp_models'  # Path to your models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>NOTE:</b></font> **Use only one of the models at a model.** \n",
    "Provided Models to compare are:\n",
    "* **Fabina Model:** \n",
    "* **Curriculum (Simple Resnet):**\n",
    "* **Layerwise (Simple Resnet):** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fabian  model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model, extract the features and set the model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fabian model\n",
    "def get_model(num_classes=200):\n",
    "    model = resnet18(weights=None)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()  # Remove maxpool\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.5),  # Dropout to avoid overfitting\n",
    "        nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from your models\n",
    "def extract_features(model_path, images):\n",
    "    model = get_model()\n",
    "    # Load the model state dictionary, mapping to CPU if necessary\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('mps')))\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        features = model(images).numpy()  # Extract features\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [os.path.join(model_dir, model_name) for model_name in [\n",
    "    \"resnet18_tinyimagenet_acuity.pth\",\n",
    "    \"resnet18_tinyimagenet_contrast.pth\",\n",
    "    \"resnet18_tinyimagenet_both.pth\",\n",
    "    \"resnet18_tinyimagenet_default.pth\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curriculum (Simple Resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model, extract the features and set the model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sohan model\n",
    "def get_resnet18(num_classes=200):\n",
    "    model = resnet18(weights=None)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from your models\n",
    "def extract_features(model_path, images):\n",
    "    model = get_resnet18()\n",
    "    # Load the model state dictionary, mapping to CPU if necessary\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('mps')))\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        features = model(images).numpy()  # Extract features\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [os.path.join(model_dir, model_name) for model_name in [\n",
    "    \"resnet18_visual_acuity_final.pth\",\n",
    "    \"resnet18_color_perception_final.pth\",\n",
    "    \"resnet18_curriculum_final.pth\",\n",
    "    \"resnet18_no_curriculum_final.pth\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layerwise (simple resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model, extract the features and set the model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sohan model\n",
    "def get_resnet18(num_classes=200):\n",
    "    model = resnet18(weights=None)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from your models\n",
    "def extract_features(model_path, images):\n",
    "    model = get_resnet18()\n",
    "    # Load the model state dictionary, mapping to CPU if necessary\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('mps')))\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        features = model(images).numpy()  # Extract features\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [os.path.join(model_dir, model_name) for model_name in [\n",
    "    \"resnet18_layerwise_acuity_final.pth\",\n",
    "    \"resnet18_layerwise_color_final.pth\",\n",
    "    \"resnet18_layerwise_final.pth\",\n",
    "    \"resnet18_no_curriculum_final.pth\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess images\n",
    "def load_images(images_dir, num_images=100):\n",
    "    images_list = os.listdir(images_dir)\n",
    "    images_list.sort()\n",
    "    images_list = images_list[:num_images]  # Select first 100 images\n",
    "\n",
    "    images = []\n",
    "    for img in images_list:\n",
    "        img_dir = os.path.join(images_dir, img)\n",
    "        img = Image.open(img_dir).convert('RGB')\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        img = transform(img)\n",
    "        images.append(img)\n",
    "    images = torch.stack(images)  # Stack into a single tensor\n",
    "    return images, images_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, images_list = load_images(images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the NED object, subject, roi to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ned_object = NED(ned_dir)\n",
    "subject = 1 # @param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate in silico fMRI responses to images\n",
    "\n",
    "Generating neural responses for images involves two steps. First, you need to choose the training dataset, encoding model type, subject and ROI, and load the corresponding fMRI encoding model using the `get_encoding_model` method.\n",
    "\n",
    "We provide fMRI encoding models for the following (NSD) ROIs:\n",
    "* **Early retinotopic visual regions:** V1, V2, V3, hV4.\n",
    "* **Body-selective regions:** EBA, FBA-2.\n",
    "* **Face-selective regions:** OFA, FFA-1, FFA-2.\n",
    "* **Place-selective regions:** OPA, PPA, RSC.\n",
    "* **Word-selective regions:** OWFA, VWFA-1, VWFA-2, mfs-words.\n",
    "* **Anatomical streams:** early, midventral, midlateral, midparietal, ventral, lateral, parietal.\n",
    "\n",
    "For more information on the NSD ROIs, please see the [NSD data manual][nsd_man].\n",
    "\n",
    "If you select encoding models trained on NSD, note that the fMRI data used to train and evaluate these encoding models were _z_-scored at each scan session. As a consequence, their generated in silico fMRI responses also live in _z_-scored space.\n",
    "\n",
    "<font color='red'><b>NOTE:</b></font> **The in silico fMRI generation will be faster using if GPU is available.**\n",
    "\n",
    "[nsd_man]: https://cvnlab.slite.page/p/X_7BBMgghj/ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ROIs to compare\n",
    "rois = [\"V1\", \"V2\", \"V3\", \"hV4\", \"EBA\", \"FBA-2\", \"OFA\", \"FFA-1\", \"FFA-2\", \"early\",\"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]  # Add more ROIs as needed\n",
    "\n",
    "# @param [\"V1\", \"V2\", \"V3\", \"hV4\", \"EBA\", \"FBA-2\", \"OFA\", \"FFA-1\", \"FFA-2\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "The results of Spearman and pearson correlation between the models and the brain regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlations:\n",
      "            acuity model contrast model curriculum model no_curriculum model\n",
      "V1              0.030257        0.01025         0.027624           -0.000826\n",
      "V2              0.042575       0.017419         0.035278            0.007411\n",
      "V3               0.02523       0.018588         0.027911            0.017775\n",
      "hV4             0.025388       0.020933         0.018436            0.004119\n",
      "EBA             0.018718       0.028583         0.029167            0.023601\n",
      "FBA-2           0.026583       0.040613         0.037381             0.05279\n",
      "OFA             0.023218       0.054792         0.038804            0.060731\n",
      "FFA-1           0.021205       0.014854         0.023721             0.02626\n",
      "FFA-2           0.010004       0.020683         0.016646            0.026218\n",
      "early           0.035069        0.04085         0.039115            0.040625\n",
      "midventral      0.022708       0.021428         0.015438            0.011751\n",
      "midlateral      0.022705       0.021953         0.026768            0.018973\n",
      "midparietal     0.028463       0.032403         0.030878            0.030289\n",
      "ventral         0.000087       -0.00108         0.010455            0.014417\n",
      "lateral         0.010741       0.011835         0.011033            0.010909\n",
      "parietal        0.013562       0.027598         0.017743             0.03692\n",
      "\n",
      "Pearson Correlations:\n",
      "            acuity model contrast model curriculum model no_curriculum model\n",
      "V1              0.065452       0.050491         0.064208            0.040375\n",
      "V2              0.073509       0.051673         0.068131             0.04174\n",
      "V3              0.057714          0.054         0.060972             0.05434\n",
      "hV4             0.053879       0.055346         0.048601            0.038349\n",
      "EBA             0.028221       0.034621         0.039946            0.034049\n",
      "FBA-2           0.034066       0.046573         0.041998            0.059805\n",
      "OFA             0.037557       0.070406          0.05514            0.077347\n",
      "FFA-1           0.025713       0.024509         0.029055            0.037105\n",
      "FFA-2           0.012265       0.028844         0.020527            0.038343\n",
      "early           0.070457       0.076749         0.076692             0.07715\n",
      "midventral      0.048084       0.047296          0.04109            0.036618\n",
      "midlateral      0.043599       0.049872         0.052474            0.044825\n",
      "midparietal     0.046519       0.050228         0.051599            0.047143\n",
      "ventral         0.011993       0.013678         0.024979            0.030263\n",
      "lateral         0.021925       0.024663         0.024812             0.02755\n",
      "parietal        0.033494       0.047351         0.038522            0.055461\n",
      "Download Spearman Correlations: ./data/spearman_correlations.csv\n",
      "Download Pearson Correlations: ./data/pearson_correlations.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize DataFrames to store results for both correlation methods\n",
    "model_names = [\"acuity model\", \"contrast model\", \"curriculum model\", \"no_curriculum model\"]\n",
    "results_spearman = pd.DataFrame(index=rois, columns=model_names)\n",
    "results_pearson = pd.DataFrame(index=rois, columns=model_names)\n",
    "\n",
    "# Iterate through each ROI and calculate similarities\n",
    "for roi in rois:\n",
    "    # Load the brain data for the current ROI\n",
    "    brain_data, _ = ned_object.load_insilico_neural_responses(\n",
    "        modality='fmri',\n",
    "        train_dataset='nsd',\n",
    "        model='fwrf',\n",
    "        imageset='nsd',\n",
    "        subject=subject,\n",
    "        roi=roi,\n",
    "        return_metadata=True\n",
    "    )\n",
    "    \n",
    "    # Calculate the brain RDM for the current ROI\n",
    "    brain_rdm = squareform(pdist(brain_data[:100], metric='correlation'))\n",
    "    model_features = [extract_features(model_path, images) for model_path in model_paths]\n",
    "    model_rdms = [squareform(pdist(features, metric='correlation')) for features in model_features]\n",
    "    \n",
    "    # Compare each model to the brain RDM using different correlations\n",
    "    for i, model_rdm in enumerate(model_rdms):\n",
    "        # Flatten the RDMs for correlation calculations\n",
    "        brain_rdm_flat = brain_rdm.flatten()\n",
    "        model_rdm_flat = model_rdm.flatten()\n",
    "\n",
    "        # Spearman correlation\n",
    "        spearman_corr, _ = spearmanr(brain_rdm_flat, model_rdm_flat)\n",
    "        results_spearman.loc[roi, model_names[i]] = spearman_corr\n",
    "        \n",
    "\n",
    "        # Pearson correlation\n",
    "        pearson_corr, _ = pearsonr(brain_rdm_flat, model_rdm_flat)\n",
    "        results_pearson.loc[roi, model_names[i]] = pearson_corr\n",
    "        \n",
    "# Save the results to CSV files\n",
    "results_spearman.to_csv(\"spearman_correlations.csv\")\n",
    "results_pearson.to_csv(\"pearson_correlations.csv\")\n",
    "\n",
    "# Display the results as tables\n",
    "print(\"Spearman Correlations:\")\n",
    "print(results_spearman)\n",
    "\n",
    "print(\"\\nPearson Correlations:\")\n",
    "print(results_pearson)\n",
    "\n",
    "# Provide download links (if running in a Jupyter-like environment)\n",
    "import shutil\n",
    "\n",
    "# Copy files to a directory accessible for download\n",
    "output_dir = \"./data/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "shutil.move(\"spearman_correlations.csv\", \"./data/spearman_correlations.csv\")\n",
    "shutil.move(\"pearson_correlations.csv\", \"./data/pearson_correlations.csv\")\n",
    "print(\"Download Spearman Correlations: ./data/spearman_correlations.csv\")\n",
    "print(\"Download Pearson Correlations: ./data/pearson_correlations.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
