{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NED Compare model to Brain Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = \"jupyter_notebook\" # @param [\"colab\", \"jupyter_notebook\"] {allow-input: true}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/gifale95/NED.git\n",
      "  Cloning https://github.com/gifale95/NED.git to /private/var/folders/sy/39dkvhds36d6q24cvfwkcymc0000gn/T/pip-req-build-jm8jnjv1\n",
      "  Running command git clone --quiet https://github.com/gifale95/NED.git /private/var/folders/sy/39dkvhds36d6q24cvfwkcymc0000gn/T/pip-req-build-jm8jnjv1\n",
      "  Resolved https://github.com/gifale95/NED.git to commit cc238e51fdd117031d1096f104e712d0a4a95f46\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: h5py in ./env/lib/python3.9/site-packages (from NED==0.2.3) (3.12.1)\n",
      "Collecting joblib (from NED==0.2.3)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting nibabel (from NED==0.2.3)\n",
      "  Using cached nibabel-5.3.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.9/site-packages (from NED==0.2.3) (1.26.4)\n",
      "Requirement already satisfied: Pillow in ./env/lib/python3.9/site-packages (from NED==0.2.3) (11.1.0)\n",
      "Requirement already satisfied: scipy in ./env/lib/python3.9/site-packages (from NED==0.2.3) (1.13.1)\n",
      "Collecting scikit-learn (from NED==0.2.3)\n",
      "  Using cached scikit_learn-1.6.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: torch in ./env/lib/python3.9/site-packages (from NED==0.2.3) (2.2.2)\n",
      "Requirement already satisfied: torchvision in ./env/lib/python3.9/site-packages (from NED==0.2.3) (0.17.2)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.9/site-packages (from NED==0.2.3) (4.67.1)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in ./env/lib/python3.9/site-packages (from nibabel->NED==0.2.3) (6.5.2)\n",
      "Requirement already satisfied: packaging>=20 in ./env/lib/python3.9/site-packages (from nibabel->NED==0.2.3) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in ./env/lib/python3.9/site-packages (from nibabel->NED==0.2.3) (4.12.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->NED==0.2.3)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.9/site-packages (from torch->NED==0.2.3) (3.17.0)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.9/site-packages (from torch->NED==0.2.3) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.9/site-packages (from torch->NED==0.2.3) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.9/site-packages (from torch->NED==0.2.3) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.9/site-packages (from torch->NED==0.2.3) (2024.12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./env/lib/python3.9/site-packages (from importlib-resources>=5.12->nibabel->NED==0.2.3) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.9/site-packages (from jinja2->torch->NED==0.2.3) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.9/site-packages (from sympy->torch->NED==0.2.3) (1.3.0)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached nibabel-5.3.2-py3-none-any.whl (3.3 MB)\n",
      "Using cached scikit_learn-1.6.1-cp39-cp39-macosx_10_9_x86_64.whl (12.1 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: NED\n",
      "  Building wheel for NED (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for NED: filename=NED-0.2.3-py3-none-any.whl size=47191 sha256=e7fcf9b09e9415d923a908b70995971efa3acead548a8899fc38f723ac76f966\n",
      "  Stored in directory: /private/var/folders/sy/39dkvhds36d6q24cvfwkcymc0000gn/T/pip-ephem-wheel-cache-bvwox6y4/wheels/9d/fd/4d/b25cf9910b4996a79b4d35d5dc4c5de9fa76deecf79742c13c\n",
      "Successfully built NED\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, nibabel, NED\n",
      "Successfully installed NED-0.2.3 joblib-1.4.2 nibabel-5.3.2 scikit-learn-1.6.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U git+https://github.com/gifale95/NED.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from ned.ned import NED\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms as trn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import spearmanr, pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform == 'jupyter_notebook':\n",
    "    ned_dir = './neural_encoding_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = os.path.join(ned_dir, 'ned_tutorials', 'tutorial_images')\n",
    "model_dir = './cvp_models'  # Path to your models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>NOTE:</b></font> **Use only one of the models at a model.** \n",
    "Provided Models to compare are:\n",
    "* **Fabina Model:** \n",
    "* **Curriculum (Simple Resnet):**\n",
    "* **Layerwise (Simple Resnet):** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fabian  model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model, extract the features and set the model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fabian model\n",
    "def get_model(num_classes=200):\n",
    "    model = resnet18(weights=None)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()  # Remove maxpool\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.5),  # Dropout to avoid overfitting\n",
    "        nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from your models\n",
    "def extract_features(model_path, images):\n",
    "    model = get_model()\n",
    "    # Load the model state dictionary, mapping to CPU if necessary\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('mps')))\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        features = model(images).numpy()  # Extract features\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [os.path.join(model_dir, model_name) for model_name in [\n",
    "    \"resnet18_tinyimagenet_acuity.pth\",\n",
    "    \"resnet18_tinyimagenet_contrast.pth\",\n",
    "    \"resnet18_tinyimagenet_both.pth\",\n",
    "    \"resnet18_tinyimagenet_default.pth\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curriculum (Simple Resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model, extract the features and set the model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sohan model\n",
    "def get_resnet18(num_classes=200):\n",
    "    model = resnet18(weights=None)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from your models\n",
    "def extract_features(model_path, images):\n",
    "    model = get_resnet18()\n",
    "    # Load the model state dictionary, mapping to CPU if necessary\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('mps')))\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        features = model(images).numpy()  # Extract features\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [os.path.join(model_dir, model_name) for model_name in [\n",
    "    \"resnet18_visual_acuity_final.pth\",\n",
    "    \"resnet18_color_perception_final.pth\",\n",
    "    \"resnet18_curriculum_final.pth\",\n",
    "    \"resnet18_no_curriculum_final.pth\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layerwise (simple resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model, extract the features and set the model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sohan model\n",
    "def get_resnet18(num_classes=200):\n",
    "    model = resnet18(weights=None)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from your models\n",
    "def extract_features(model_path, images):\n",
    "    model = get_resnet18()\n",
    "    # Load the model state dictionary, mapping to CPU if necessary\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('mps')))\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        features = model(images).numpy()  # Extract features\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [os.path.join(model_dir, model_name) for model_name in [\n",
    "    \"resnet18_layerwise_acuity_final.pth\",\n",
    "    \"resnet18_layerwise_color_final.pth\",\n",
    "    \"resnet18_layerwise_final.pth\",\n",
    "    \"resnet18_no_curriculum_final.pth\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess images\n",
    "def load_images(images_dir, num_images=100):\n",
    "    images_list = os.listdir(images_dir)\n",
    "    images_list.sort()\n",
    "    images_list = images_list[:num_images]  # Select first 100 images\n",
    "\n",
    "    images = []\n",
    "    for img in images_list:\n",
    "        img_dir = os.path.join(images_dir, img)\n",
    "        img = Image.open(img_dir).convert('RGB')\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        img = transform(img)\n",
    "        images.append(img)\n",
    "    images = torch.stack(images)  # Stack into a single tensor\n",
    "    return images, images_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, images_list = load_images(images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the NED object, subject, roi to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ned_object = NED(ned_dir)\n",
    "subject = 1 # @param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate in silico fMRI responses to images\n",
    "\n",
    "Generating neural responses for images involves two steps. First, you need to choose the training dataset, encoding model type, subject and ROI, and load the corresponding fMRI encoding model using the `get_encoding_model` method.\n",
    "\n",
    "We provide fMRI encoding models for the following (NSD) ROIs:\n",
    "* **Early retinotopic visual regions:** V1, V2, V3, hV4.\n",
    "* **Body-selective regions:** EBA, FBA-2.\n",
    "* **Face-selective regions:** OFA, FFA-1, FFA-2.\n",
    "* **Place-selective regions:** OPA, PPA, RSC.\n",
    "* **Word-selective regions:** OWFA, VWFA-1, VWFA-2, mfs-words.\n",
    "* **Anatomical streams:** early, midventral, midlateral, midparietal, ventral, lateral, parietal.\n",
    "\n",
    "For more information on the NSD ROIs, please see the [NSD data manual][nsd_man].\n",
    "\n",
    "If you select encoding models trained on NSD, note that the fMRI data used to train and evaluate these encoding models were _z_-scored at each scan session. As a consequence, their generated in silico fMRI responses also live in _z_-scored space.\n",
    "\n",
    "<font color='red'><b>NOTE:</b></font> **The in silico fMRI generation will be faster using if GPU is available.**\n",
    "\n",
    "[nsd_man]: https://cvnlab.slite.page/p/X_7BBMgghj/ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ROIs to compare\n",
    "rois = [\"V1\", \"V2\", \"V3\", \"hV4\", \"EBA\", \"FBA-2\", \"OFA\", \"FFA-1\", \"FFA-2\", \"early\",\"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]  # Add more ROIs as needed\n",
    "\n",
    "# @param [\"V1\", \"V2\", \"V3\", \"hV4\", \"EBA\", \"FBA-2\", \"OFA\", \"FFA-1\", \"FFA-2\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "The results of Spearman and pearson correlation between the models and the brain regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlations:\n",
      "            acuity model contrast model curriculum model no_curriculum model\n",
      "V1              0.030257        0.01025         0.027624           -0.000826\n",
      "V2              0.042575       0.017419         0.035278            0.007411\n",
      "V3               0.02523       0.018588         0.027911            0.017775\n",
      "hV4             0.025388       0.020933         0.018436            0.004119\n",
      "EBA             0.018718       0.028583         0.029167            0.023601\n",
      "FBA-2           0.026583       0.040613         0.037381             0.05279\n",
      "OFA             0.023218       0.054792         0.038804            0.060731\n",
      "FFA-1           0.021205       0.014854         0.023721             0.02626\n",
      "FFA-2           0.010004       0.020683         0.016646            0.026218\n",
      "early           0.035069        0.04085         0.039115            0.040625\n",
      "midventral      0.022708       0.021428         0.015438            0.011751\n",
      "midlateral      0.022705       0.021953         0.026768            0.018973\n",
      "midparietal     0.028463       0.032403         0.030878            0.030289\n",
      "ventral         0.000087       -0.00108         0.010455            0.014417\n",
      "lateral         0.010741       0.011835         0.011033            0.010909\n",
      "parietal        0.013562       0.027598         0.017743             0.03692\n",
      "\n",
      "Pearson Correlations:\n",
      "            acuity model contrast model curriculum model no_curriculum model\n",
      "V1              0.065452       0.050491         0.064208            0.040375\n",
      "V2              0.073509       0.051673         0.068131             0.04174\n",
      "V3              0.057714          0.054         0.060972             0.05434\n",
      "hV4             0.053879       0.055346         0.048601            0.038349\n",
      "EBA             0.028221       0.034621         0.039946            0.034049\n",
      "FBA-2           0.034066       0.046573         0.041998            0.059805\n",
      "OFA             0.037557       0.070406          0.05514            0.077347\n",
      "FFA-1           0.025713       0.024509         0.029055            0.037105\n",
      "FFA-2           0.012265       0.028844         0.020527            0.038343\n",
      "early           0.070457       0.076749         0.076692             0.07715\n",
      "midventral      0.048084       0.047296          0.04109            0.036618\n",
      "midlateral      0.043599       0.049872         0.052474            0.044825\n",
      "midparietal     0.046519       0.050228         0.051599            0.047143\n",
      "ventral         0.011993       0.013678         0.024979            0.030263\n",
      "lateral         0.021925       0.024663         0.024812             0.02755\n",
      "parietal        0.033494       0.047351         0.038522            0.055461\n",
      "Download Spearman Correlations: ./data/spearman_correlations.csv\n",
      "Download Pearson Correlations: ./data/pearson_correlations.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize DataFrames to store results for both correlation methods\n",
    "model_names = [\"acuity model\", \"contrast model\", \"curriculum model\", \"no_curriculum model\"]\n",
    "results_spearman = pd.DataFrame(index=rois, columns=model_names)\n",
    "results_pearson = pd.DataFrame(index=rois, columns=model_names)\n",
    "\n",
    "# Iterate through each ROI and calculate similarities\n",
    "for roi in rois:\n",
    "    # Load the brain data for the current ROI\n",
    "    brain_data, _ = ned_object.load_insilico_neural_responses(\n",
    "        modality='fmri',\n",
    "        train_dataset='nsd',\n",
    "        model='fwrf',\n",
    "        imageset='nsd',\n",
    "        subject=subject,\n",
    "        roi=roi,\n",
    "        return_metadata=True\n",
    "    )\n",
    "    \n",
    "    # Calculate the brain RDM for the current ROI\n",
    "    brain_rdm = squareform(pdist(brain_data[:100], metric='correlation'))\n",
    "    model_features = [extract_features(model_path, images) for model_path in model_paths]\n",
    "    model_rdms = [squareform(pdist(features, metric='correlation')) for features in model_features]\n",
    "    \n",
    "    # Compare each model to the brain RDM using different correlations\n",
    "    for i, model_rdm in enumerate(model_rdms):\n",
    "        # Flatten the RDMs for correlation calculations\n",
    "        brain_rdm_flat = brain_rdm.flatten()\n",
    "        model_rdm_flat = model_rdm.flatten()\n",
    "\n",
    "        # Spearman correlation\n",
    "        spearman_corr, _ = spearmanr(brain_rdm_flat, model_rdm_flat)\n",
    "        results_spearman.loc[roi, model_names[i]] = spearman_corr\n",
    "        \n",
    "\n",
    "        # Pearson correlation\n",
    "        pearson_corr, _ = pearsonr(brain_rdm_flat, model_rdm_flat)\n",
    "        results_pearson.loc[roi, model_names[i]] = pearson_corr\n",
    "        \n",
    "# Save the results to CSV files\n",
    "results_spearman.to_csv(\"spearman_correlations.csv\")\n",
    "results_pearson.to_csv(\"pearson_correlations.csv\")\n",
    "\n",
    "# Display the results as tables\n",
    "print(\"Spearman Correlations:\")\n",
    "print(results_spearman)\n",
    "\n",
    "print(\"\\nPearson Correlations:\")\n",
    "print(results_pearson)\n",
    "\n",
    "# Provide download links (if running in a Jupyter-like environment)\n",
    "import shutil\n",
    "\n",
    "# Copy files to a directory accessible for download\n",
    "output_dir = \"./data/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "shutil.move(\"spearman_correlations.csv\", \"./data/spearman_correlations.csv\")\n",
    "shutil.move(\"pearson_correlations.csv\", \"./data/pearson_correlations.csv\")\n",
    "print(\"Download Spearman Correlations: ./data/spearman_correlations.csv\")\n",
    "print(\"Download Pearson Correlations: ./data/pearson_correlations.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m roi \u001b[38;5;129;01min\u001b[39;00m rois:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Load the brain data for the current ROI\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     brain_data, _ \u001b[38;5;241m=\u001b[39m ned_object\u001b[38;5;241m.\u001b[39mload_insilico_neural_responses(\n\u001b[1;32m     16\u001b[0m         modality\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfmri\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m         train_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnsd\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m         return_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[0;32m---> 25\u001b[0m     model_features \u001b[38;5;241m=\u001b[39m [extract_features(model_path, images) \u001b[38;5;28;01mfor\u001b[39;00m model_path \u001b[38;5;129;01min\u001b[39;00m model_paths]\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Calculate the brain RDM for the current ROI\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     brain_rdm \u001b[38;5;241m=\u001b[39m squareform(pdist(brain_data[:\u001b[38;5;241m100\u001b[39m], metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrelation\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m roi \u001b[38;5;129;01min\u001b[39;00m rois:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Load the brain data for the current ROI\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     brain_data, _ \u001b[38;5;241m=\u001b[39m ned_object\u001b[38;5;241m.\u001b[39mload_insilico_neural_responses(\n\u001b[1;32m     16\u001b[0m         modality\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfmri\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m         train_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnsd\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m         return_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[0;32m---> 25\u001b[0m     model_features \u001b[38;5;241m=\u001b[39m [\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m model_path \u001b[38;5;129;01min\u001b[39;00m model_paths]\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Calculate the brain RDM for the current ROI\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     brain_rdm \u001b[38;5;241m=\u001b[39m squareform(pdist(brain_data[:\u001b[38;5;241m100\u001b[39m], metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrelation\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(model_path, images)\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set to evaluation mode\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[0;32m~/Desktop/Sohan/FAU/Courses/24WS/Computational Visual Perception/Github/Part_3/ned_fmri/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Sohan/FAU/Courses/24WS/Computational Visual Perception/Github/Part_3/ned_fmri/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Sohan/FAU/Courses/24WS/Computational Visual Perception/Github/Part_3/ned_fmri/env/lib/python3.9/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Sohan/FAU/Courses/24WS/Computational Visual Perception/Github/Part_3/ned_fmri/env/lib/python3.9/site-packages/torchvision/models/resnet.py:276\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[0;32m--> 276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    279\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Sohan/FAU/Courses/24WS/Computational Visual Perception/Github/Part_3/ned_fmri/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Sohan/FAU/Courses/24WS/Computational Visual Perception/Github/Part_3/ned_fmri/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Sohan/FAU/Courses/24WS/Computational Visual Perception/Github/Part_3/ned_fmri/env/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Sohan/FAU/Courses/24WS/Computational Visual Perception/Github/Part_3/ned_fmri/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Sohan/FAU/Courses/24WS/Computational Visual Perception/Github/Part_3/ned_fmri/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Sohan/FAU/Courses/24WS/Computational Visual Perception/Github/Part_3/ned_fmri/env/lib/python3.9/site-packages/torchvision/models/resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     90\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/Desktop/Sohan/FAU/Courses/24WS/Computational Visual Perception/Github/Part_3/ned_fmri/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Sohan/FAU/Courses/24WS/Computational Visual Perception/Github/Part_3/ned_fmri/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Sohan/FAU/Courses/24WS/Computational Visual Perception/Github/Part_3/ned_fmri/env/lib/python3.9/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Sohan/FAU/Courses/24WS/Computational Visual Perception/Github/Part_3/ned_fmri/env/lib/python3.9/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the ROIs to compare\n",
    "rois = [\"V1\", \"V2\", \"V3\", \"VWFA-1\", \"VWFA-2\", \"ventral\"]  # Add more ROIs as needed\n",
    "\n",
    "# @param [\"V1\", \"V2\", \"V3\", \"hV4\", \"EBA\", \"FBA-2\", \"OFA\", \"FFA-1\", \"FFA-2\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]\n",
    "\n",
    "# Initialize a DataFrame to store the results\n",
    "model_names = [\"curriculum model\", \"acuity model\", \"diff_feature model\", \"no_curriculum model\"]\n",
    "results = pd.DataFrame(index=model_names, columns=rois)\n",
    "\n",
    "# Iterate through each ROI and calculate similarities\n",
    "for roi in rois:\n",
    "    # Load the brain data for the current ROI\n",
    "    brain_data, _ = ned_object.load_insilico_neural_responses(\n",
    "        modality='fmri',\n",
    "        train_dataset='nsd',\n",
    "        model='fwrf',\n",
    "        imageset='nsd',\n",
    "        subject=subject,\n",
    "        roi=roi,\n",
    "        return_metadata=True\n",
    "    )\n",
    "    \n",
    "    model_features = [extract_features(model_path, images) for model_path in model_paths]\n",
    "    \n",
    "    # Calculate the brain RDM for the current ROI\n",
    "    brain_rdm = squareform(pdist(brain_data[:100], metric='correlation'))\n",
    "    model_rdms = [squareform(pdist(features, metric='correlation')) for features in model_features]\n",
    "    \n",
    "    # Compare each model to the brain RDM\n",
    "    for i, model_rdm in enumerate(model_rdms):\n",
    "        similarity = np.corrcoef(brain_rdm.flatten(), model_rdm.flatten())[0, 1]\n",
    "        results.loc[model_names[i], roi] = similarity\n",
    "\n",
    "# Display the results as a table\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-generated brain data\n",
    "brain_data, _ = ned_object.load_insilico_neural_responses(\n",
    "    modality='fmri',\n",
    "    train_dataset='nsd',\n",
    "    model='fwrf',\n",
    "    imageset='nsd',\n",
    "    subject=subject,\n",
    "    roi=roi,\n",
    "    return_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = [extract_features(model_path, images) for model_path in model_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Calculate RDMs for your models and brain data (using correlation distance)\n",
    "model_rdms = [squareform(pdist(features, metric='correlation')) for features in model_features]\n",
    "brain_rdm = squareform(pdist(brain_data[:100], metric='correlation'))  # Using first 100 images\n",
    "\n",
    "# Compare model RDMs to brain RDM (e.g., using correlation)\n",
    "for i, model_rdm in enumerate(model_rdms):\n",
    "    similarity = np.corrcoef(brain_rdm.flatten(), model_rdm.flatten())[0, 1]\n",
    "    print(f\"Similarity between model {i+1} and brain: {similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
